"""
This script is to valid the crafted adversarial images on held-out data
"""
import os
import csv
import torch
import random
import argparse
import torchvision
import numpy as np
import pandas as pd
from io import BytesIO
from PIL import Image
from tqdm import tqdm
from transformers import AutoProcessor, LlavaForConditionalGeneration, CLIPProcessor, CLIPModel


def parse_args():
    parser = argparse.ArgumentParser()

    # parameters related to attack
    parser.add_argument("--attacker", type=str, default='MI', help="attack method")
    parser.add_argument("--pixel_attack", action='store_true', default=False, help="whether to enable pixel attack")
    parser.add_argument("--border", type=int, default=6, help="border width for border attack")
    parser.add_argument("--size", type=int, default=336, help="image size of adversarial examples")
    parser.add_argument("--epochs", type=int, default=100, help="number of optimization epochs")
    parser.add_argument("--epsilon", type=int, default=8, help="perturbation budget for pixel attack")
    parser.add_argument("--eta", type=float, default=0.5, help="step size")
    parser.add_argument("--seed", type=int, default=42, help="random seed")
    parser.add_argument("--mu", type=float, default=0.95, help="momentum factor for vlm attack")
    parser.add_argument("--lambda_r", type=float, default=1.0, help="RAG loss weight")
    parser.add_argument("--unconstrained", action='store_true', default=False, help="whether to enable unconstrained optimization, true for border attack")
    
    parser.add_argument("--root", type=str, default="experiments", help="folder to save results")
    parser.add_argument("--div", type=str, default="high", help="scenario for textual chat diversity, low or high")
    parser.add_argument("--dtype", type=str, default="bf16", help="data type for models and data, bf16 or fp16 or fp32")
    parser.add_argument("--ensemble_size", type=int, default=512, help="ensemble sample size")
    parser.add_argument("--valid_epoch", type=int, default=10, help="per epoch for validation")
    parser.add_argument("--valid_threshold", type=float, default=0.99, help="validation threshold for jailbreak success rate")

    # parameters related to input augmentation
    parser.add_argument("--prob_random_flip", type=float, default=0.0, help="probability of enabling random flip during the optimization")
    parser.add_argument("--enable_random_resize", action='store_true', default=False, help="whether to enable random resize during the optimization")
    parser.add_argument("--upper_random_resize", type=int, default=448, help="upper image resize for random resize")
    parser.add_argument("--lower_random_resize", type=int, default=224, help="lower image resize for random resize")
    parser.add_argument("--prob_random_jpeg", type=float, default=0.0, help="probability of enabling random jpeg during the optimization")

    # parameters related to models
    parser.add_argument("--vlm", type=str, default='llava-hf/llava-1.5-7b-hf', help="vlm model path")
    parser.add_argument("--rag", type=str, default='openai/clip-vit-large-patch14', help="rag model path")
    parser.add_argument("--max_new_tokens", type=int, default=64, help="maximum of new tokens generated by vlm")
    parser.add_argument("--do_sample", action='store_true', default=False)
    args = parser.parse_args()
    return args


def set_seeds(seed):    
    random.seed(seed)
    np.random.seed(seed)
    
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def clamp(x, ori_x, epsilon, norm='Linf'):
    B = x.shape[0]
    if norm == 'Linf':
        x = torch.clamp(x, min=ori_x - epsilon, max=ori_x + epsilon)
    elif norm == 'L2':
        difference = x - ori_x
        distance = torch.norm(difference.view(B, -1), p=2, dim=1)
        mask = distance > epsilon
        if torch.sum(mask) > 0:
            difference[mask] = difference[mask] / distance[mask].view(torch.sum(mask), 1, 1, 1) * epsilon
            x = ori_x + difference
    x = torch.clamp(x, min=0, max=1)
    return x


def convertToJpeg(im, quality=75):
    with BytesIO() as f:
        im.save(f, format='JPEG', quality=quality)
        f.seek(0)
        im = Image.open(f)
        im.load()
        return im

# MI Attack
def attack_validate(args, epoch, raw_image, test_prompts, target, test_queries, model, processor, clip_model, clip_processor, device, save_dir, dtype):
    max_new_tokens = args.max_new_tokens
    do_sample = args.do_sample
    upper_random_resize = args.upper_random_resize
    lower_random_resize = args.lower_random_resize
    enable_random_resize = args.enable_random_resize
    prob_random_flip = args.prob_random_flip
    prob_random_jpeg = args.prob_random_jpeg
    jpeg_quality_factors = [75]

    # valid log
    valid_log = os.path.join(save_dir, f"valid_epoch{epoch}.log")
    with open(valid_log, 'a') as f:
            f.write(str(args))  # write into configs
            f.write("\n")
    
    random_flip = torchvision.transforms.RandomHorizontalFlip(p=prob_random_flip)

    # start evaluation on test dataset
    test_accuracy = 0.0
    for index in tqdm(range(len(test_prompts))):
        # fetch prompt and query
        prompt = test_prompts[index]
        
        # random jpeg + horizontal random flip + random resize
        image = raw_image
        if random.random() < prob_random_jpeg:   # enable random jpeg compression
            random_jpeg_index = random.randint(0, len(jpeg_quality_factors)-1)
            image = convertToJpeg(image, quality=jpeg_quality_factors[random_jpeg_index])
        image = random_flip(image)
        if enable_random_resize:
            random_resize = random.randint(lower_random_resize, upper_random_resize)
            image = image.resize((random_resize, random_resize))

        
        # generate response
        inputs = processor(prompt, image, return_tensors='pt').to(device, dtype)
        outputs = model.generate(**inputs,
                                max_new_tokens=max_new_tokens, 
                                do_sample=do_sample)
        outputs = processor.decode(outputs[0, len(inputs.input_ids[0]):], skip_special_tokens=True)
        outputs = outputs + "</s>"

        with open(valid_log, 'a') as f:
            f.write("**********************Sampling**********************" + "\n" + outputs + "\n")
        
        test_accuracy += (outputs == target)
    
    message = f"load prompts from test dataset and test accuracy is {test_accuracy/len(test_prompts) if len(test_prompts)>0 else test_accuracy}"
    print(message)
    with open(valid_log, 'a') as f:
        f.write(message + "\n")

    csv_path = os.path.join(save_dir, "similarity", f"valid_epoch{epoch}.csv")
    sim_data = []

    # start evaluation on test dataset
    test_similarity = []
    for query in tqdm(test_queries):
        clip_text_input = clip_processor.tokenizer(
                query,
                padding="max_length",
                max_length=77,
                truncation=True,
                return_tensors="pt",
            ).input_ids.to(device)
        
        # random jpeg + horizontal random flip + random resize
        image = raw_image
        if random.random() < prob_random_jpeg:   # enable random jpeg compression
            random_jpeg_index = random.randint(0, len(jpeg_quality_factors)-1)
            image = convertToJpeg(image, quality=jpeg_quality_factors[random_jpeg_index])
        image = random_flip(image)
        if enable_random_resize:
            random_resize = random.randint(lower_random_resize, upper_random_resize)
            image = image.resize((random_resize, random_resize))

        clip_img_input = clip_processor.image_processor.preprocess(image, return_tensors='pt').pixel_values.to(device)
        text_embeddings_clip = clip_model.get_text_features(clip_text_input)
        text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)
        image_embeddings_clip = clip_model.get_image_features(clip_img_input)
        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)
        similarity = text_embeddings_clip @ image_embeddings_clip.T
        test_similarity.append(similarity.item())
        sim_data.append({"query": query, "similarity": similarity.item()})
    message = f"load queries from test dataset and test similarity is mean: {sum(test_similarity)/len(test_queries) if len(test_queries)>0 else 0.0}, min: {min(test_similarity) if len(test_similarity)>0 else 0.0}"
    print(message)
    with open(valid_log, 'a') as f:
        f.write(message + "\n")
    
    # save csv
    df = pd.DataFrame.from_records(sim_data)
    df.to_csv(csv_path, index=False)
    return test_accuracy, min(test_similarity)

            
def main():
    # load arguments
    args = parse_args()

    # define data type
    dtype = torch.float32
    if args.dtype == "fp16":
        dtype = torch.float16
    elif args.dtype == "bf16":
        dtype = torch.bfloat16
    
    # prepare experimental folder
    if args.pixel_attack:
        exp_dir = f"{args.root}/{args.div}_diversity_size{args.size}_pixel_epsilon{args.epsilon}_255"
    else:
        exp_dir = f"{args.root}/{args.div}_diversity_size{args.size}_border_width{args.border}"
    if args.unconstrained:
        save_dir = os.path.join(exp_dir, f"ensemble_size{args.ensemble_size}_unconstrained_eta{args.eta}_255_lambda_r_{args.lambda_r}_mu{args.mu}")
    else:
        save_dir = os.path.join(exp_dir, f"ensemble_size{args.ensemble_size}_constrained_eta{args.eta}_255_lambda_r_{args.lambda_r}_mu{args.mu}")
    os.makedirs(os.path.join(save_dir, "similarity"), exist_ok=True)

    device = "cuda"
    # load model
    model_id = args.vlm
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        low_cpu_mem_usage=True, 
        device_map = 'auto',
    )
    model.eval()
    model.requires_grad = False

    processor = AutoProcessor.from_pretrained(model_id)

    # load CLIP model for RAG
    clip_model = CLIPModel.from_pretrained(args.rag, torch_dtype=dtype).to(device)
    clip_processor = CLIPProcessor.from_pretrained(args.rag)
    clip_model.eval()
    clip_model.requires_grad = False

    # load target
    target_path = "demo.txt"
    with open(target_path, "r") as f:
        lines = f.readlines()
    targets = [line.split('\n')[0] for line in lines if len(line) > 0]
    target = targets[0] + "</s>"
    print(f"Use string {target} as target")

    # load prompts and queries        
    rows = []
    with open(f"simulation_{args.div}.csv", 'r') as file:
        csvreader = csv.reader(file)
        header = next(csvreader)
        for row in csvreader:
            rows.append(row)
    queries = [row[5] for row in rows]
    active_prompts = [row[6] for row in rows]
    passive_prompts = [row[8] for row in rows]
    print(f"Load data from simulation_{args.div}.csv")

    # set seed
    set_seeds(seed=args.seed)
    # split train and test data
    data = list(zip(active_prompts, passive_prompts, queries))
    random.shuffle(data)
    active_prompts_shuffle, passive_prompts_shuffle, queries_shuffle = zip(*data)
    train_queries, test_queries = queries_shuffle[:args.ensemble_size],  queries_shuffle[args.ensemble_size:]
    train_active_prompts, test_active_prompts = active_prompts_shuffle[:args.ensemble_size], active_prompts_shuffle[args.ensemble_size:]
    train_passive_prompts, test_passive_prompts = passive_prompts_shuffle[:args.ensemble_size], passive_prompts_shuffle[args.ensemble_size:]

    # combine active and passive prompts
    train_prompts = train_active_prompts + train_passive_prompts
    test_prompts = test_active_prompts + test_passive_prompts

    # Start Attack using MI
    best_record = (0.0, 0.0)
    best_epoch = 0
    valid_threshold = 0.99
    for epoch in range(args.valid_epoch, args.epochs + args.valid_epoch, args.valid_epoch):
        # load image
        raw_image = Image.open(os.path.join(save_dir, "images", f"epoch_{epoch}_Image.png")) 
        # validation
        jsr, minclip = attack_validate(args, epoch, raw_image, test_prompts, target, test_queries, model, processor, clip_model, clip_processor, device, save_dir, dtype)
        
        if best_record[0] >= valid_threshold:
            if jsr >= valid_threshold and minclip > best_record[1]:
                best_record = (jsr, minclip)
                best_epoch = epoch
        else:
            if jsr >= best_record[0]:
                best_record = (jsr, minclip)
                best_epoch = epoch
    best_image_path = os.path.join(save_dir, "images", f"epoch_{best_epoch}_Image.png")
    new_image_path = os.path.join(save_dir, "adv_image.png")
    # save the best one
    command = f"cp {best_image_path} {new_image_path}"
    print(command)
    os.system(command)

if __name__ == "__main__":
    main()
